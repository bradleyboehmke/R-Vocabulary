---
title: "Scraping Text and Data (book)"
author: NULL
date: NULL
output:
  html_document:
    keep_md: true
---

## Scraping HTML text {#scraping_HTML_text}

Vast amount of information exists across the interminable webpages that exist online.  Much of this information are "unstructured" text that may be useful in our analyses.  This section covers the basics of scraping these texts from online sources.  Throughout this section I will illustrate how to extract different text components of webpages by dissecting the [Wikipedia page on web scraping](https://en.wikipedia.org/wiki/Web_scraping).  However, its important to first cover one of the basic components of HTML elements as we will leverage this information to pull desired information. I offer only enough insight required to begin scraping; I highly recommend [*XML and Web Technologies for Data Sciences with R*](http://www.amazon.com/XML-Web-Technologies-Data-Sciences/dp/1461478995) and [*Automated Data Collection with R*](http://www.amazon.com/Automated-Data-Collection-Practical-Scraping/dp/111883481X/ref=pd_sim_14_1?ie=UTF8&dpID=51Tm7FHxWBL&dpSrc=sims&preST=_AC_UL160_SR108%2C160_&refRID=1VJ1GQEY0VCPZW7VKANX) to learn more about HTML and XML element structures.

HTML elements are written with a start tag, an end tag, and with the content in between: `<tagname>content</tagname>`. The tags which typically contain the textual content we wish to scrape, and the tags we will leverage in the next two sections, include:

- `<h1>`, `<h2>`,...,`<h6>`: Largest heading, second largest heading, etc.
- `<p>`: Paragraph elements
- `<ul>`: Unordered bulleted list
- `<ol>`: Ordered list
- `<li>`: Individual List item
- `<div>`: Division or section
- `<table>`: Table

For example, text in paragraph form that you see online is wrapped with the HTML paragraph tag `<p>` as in:

{linenos=off}
```{r, eval=FALSE}
<p>
This paragraph represents
a typical text paragraph
in HTML form
</p>
```   

It is through these tags that we can start to extract textual components (also referred to as nodes) of HTML webpages.

### Scraping HTML Nodes
To scrape online text we'll make use of the relatively newer [`rvest`](https://cran.r-project.org/web/packages/rvest/index.html) package. `rvest` was created by the RStudio team inspired by libraries such as [beautiful soup](http://www.crummy.com/software/BeautifulSoup/) which has greatly simplified web scraping. `rvest` provides multiple functionalities; however, in this section we will focus only on extracting HTML text with `rvest`. Its important to note that `rvest` makes use of of the pipe operator (`%>%`) developed through the [`magrittr` package](https://cran.r-project.org/web/packages/magrittr/index.html). If you are not familiar with the functionality of `%>%` I recommend you jump to the chapter on [Simplifying Your Code with `%>%`](#pipe) so that you have a better understanding of what's going on with the code.

To extract text from a webpage of interest, we specify what HTML elements we want to select by using `html_nodes()`.  For instance, if we want to scrape the primary heading for the [Web Scraping Wikipedia webpage](https://en.wikipedia.org/wiki/Web_scraping) we simply identify the `<h1>` node as the node we want to select.  `html_nodes()` will identify all `<h1>` nodes on the webpage and return the HTML element.  In our example we see there is only one `<h1>` node on this webpage.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(rvest)
```

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
library(rvest)

scraping_wiki <- read_html("https://en.wikipedia.org/wiki/Web_scraping")

scraping_wiki %>%
        html_nodes("h1")
```

To extract only the heading text for this `<h1>` node, and not include all the HTML syntax we use `html_text()` which returns the heading text we see at the top of the [Web Scraping Wikipedia page](https://en.wikipedia.org/wiki/Web_scraping).

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
scraping_wiki %>%
        html_nodes("h1") %>%
        html_text()
```

If we want to identify all the second level headings on the webpage we follow the same process but instead select the `<h2>` nodes.  In this example we see there are 10 second level headings on the [Web Scraping Wikipedia page](https://en.wikipedia.org/wiki/Web_scraping).

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
scraping_wiki %>%
        html_nodes("h2") %>%
        html_text()
```

Next, we can move on to extracting much of the text on this webpage which is in paragraph form.  We can follow the same process illustrated above but instead we'll select all `<p>`  nodes.  This selects the 17 paragraph elements from the web page; which we can examine by subsetting the list `p_nodes` to see the first line of each paragraph along with the HTML syntax. Just as before, to extract the text from these nodes and coerce them to a character string we simply apply `html_text()`.

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
p_nodes <- scraping_wiki %>% 
        html_nodes("p")

length(p_nodes)

p_nodes[1:6]


p_text <- scraping_wiki %>%
        html_nodes("p") %>%
        html_text()

p_text[1]
```

Not too bad; however, we may not have captured all the text that we were hoping for.  Since we extracted text for all `<p>` nodes, we collected all identified paragraph text; however, this does not capture the text in the bulleted lists.  For example, when you look at the [Web Scraping Wikipedia page](https://en.wikipedia.org/wiki/Web_scraping) you will notice a significant amount of text in bulleted list format following the third paragraph under the **[Techniques](https://en.wikipedia.org/wiki/Web_scraping#Techniques)** heading.  If we look at our data we'll see that that the text in this list format are not capture between the two paragraphs:

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
p_text[5]

p_text[6]
```

This is because the text in this list format are contained in `<ul>` nodes. To capture the text in lists, we can use the same steps as above but we select specific nodes which represent HTML lists components. We can approach extracting list text two ways.  

First, we can pull all list elements (`<ul>`).  When scraping all `<ul>` text, the resulting data structure will be a character string vector with each element representing a single list consisting of all list items in that list.  In our running example there are 21 list elements as shown in the example that follows.  You can see the first list scraped is the table of contents and the second list scraped is the list in the [Techniques](https://en.wikipedia.org/wiki/Web_scraping#Techniques) section.

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
ul_text <- scraping_wiki %>%
        html_nodes("ul") %>%
        html_text()

length(ul_text)

ul_text[1]

# read the first 200 characters of the second list
substr(ul_text[2], start = 1, stop = 200)
```

An alternative approach is to pull all `<li>` nodes.  This will pull the text contained in each list item for all the lists.  In our running example there's 146 list items that we can extract from this Wikipedia page.  The first eight list items are the list of contents we see towards the top of the page. List items 9-17 are the list elements contained in the "[Techniques](https://en.wikipedia.org/wiki/Web_scraping#Techniques)" section, list items 18-44 are the items listed under the "[Notable Tools](https://en.wikipedia.org/wiki/Web_scraping#Notable_tools)" section, and so on.  

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
li_text <- scraping_wiki %>%
        html_nodes("li") %>%
        html_text()

length(li_text)

li_text[1:8]
```

At this point we may believe we have all the text desired and proceed with joining the paragraph (`p_text`) and list (`ul_text` or `li_text`) character strings and then perform the desired textual analysis.  However, we may now have captured *more* text than we were hoping for.  For example, by scraping all lists we are also capturing the listed links in the left margin of the webpage. If we look at the 104-136 list items that we scraped, we'll see that these texts correspond to the left margin text. 

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
li_text[104:136]
```

If we desire to scrape every piece of text on the webpage than this won't be of concern.  In fact, if we want to scrape all the text regardless of the content they represent there is an easier approach.  We can capture all the content to include text in paragraph (`<p>`), lists (`<ul>`, `<ol>`, and `<li>`), and even data in tables (`<table>`) by using `<div>`.  This is because these other elements are usually a subsidiary of an HTML division or section so pulling all `<div>` nodes will extract all text contained in that division or section regardless if it is also contained in a paragraph or list.

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
all_text <- scraping_wiki %>%
        html_nodes("div") %>% 
        html_text()
```

### Scraping Specific HTML Nodes {#scraping_specific_nodes}
However, if we are concerned only with specific content on the webpage then we need to make our HTML node selection process a little more focused.  To do this we, we can use our browser's developer tools to examine the webpage we are scraping and get more details on specific nodes of interest.  If you are using Chrome or Firefox you can open the developer tools by clicking F12 (Cmd + Opt + I for Mac) or for Safari you would use Command-Option-I. An additional option which is recommended by Hadley Wickham is to use [selectorgadget.com](http://selectorgadget.com/), a Chrome extension, to help identify the web page elements you need[^selector]. 

Once the developers tools are opened your primary concern is with the element selector. This is located in the top lefthand corner of the developers tools window. 

<center>
<img src="images/element_selector.jpg" alt="Element Selector Tool">
</center> 
<br>

Once you've selected the element selector you can now scroll over the elements of the webpage which will cause each element you scroll over to be highlighted.  Once you've identified the element you want to focus on, select it. This will cause the element to be identified in the developer tools window. For example, if I am only interested in the main body of the Web Scraping content on the Wikipedia page then I would select the element that highlights the entire center component of the webpage.  This highlights the corresponding element `<div id="bodyContent" class="mw-body-content">` in the developer tools window as the following illustrates.

<center>
<img src="images/body_content_selected.png" alt="Body Content Selected">
</center>  
<br>

I can now use this information to select and scrape all the text from this specific `<div>` node by calling the ID name ("#mw-content-text") in `html_nodes()`[^selector2].  As you can see below, the text that is scraped begins with the first line in the main body of the Web Scraping content and ends with the text in the [See Also](https://en.wikipedia.org/wiki/Web_scraping#See_also_2) section which is the last bit of text directly pertaining to Web Scraping on the webpage. Explicitly, we have pulled the specific text associated with the web content we desire.

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
body_text <- scraping_wiki %>%
        html_nodes("#mw-content-text") %>% 
        html_text()

# read the first 207 characters
substr(body_text, start = 1, stop = 207)

# read the last 73 characters
substr(body_text, start = nchar(body_text)-73, stop = nchar(body_text))
```

Using the developer tools approach allows us to be as specific as we desire.  We can identify the class name for a specific HTML element and scrape the text for only that node rather than all the other elements with similar tags. This allows us to scrape the main body of content as we just illustrated or we can also identify specific headings, paragraphs, lists, and list components if we desire to scrape only these specific pieces of text: 

{linenos=off}
```{r, collapse=TRUE, warning=FALSE, message=FALSE}
# Scraping a specific heading
scraping_wiki %>%
        html_nodes("#Techniques") %>% 
        html_text()

# Scraping a specific paragraph
scraping_wiki %>%
        html_nodes("#mw-content-text > p:nth-child(20)") %>% 
        html_text()

# Scraping a specific list
scraping_wiki %>%
        html_nodes("#mw-content-text > div:nth-child(22)") %>% 
        html_text()

# Scraping a specific reference list item
scraping_wiki %>%
        html_nodes("#cite_note-22") %>% 
        html_text()
```

### Cleaning up
With any webscraping activity, especially involving text, there is likely to be some clean-up involved. For example, in the previous example we saw that we can specifically pull the list of [**Notable Tools**](https://en.wikipedia.org/wiki/Web_scraping#Notable_tools); however, you can see that in between each list item rather than a space there contains one or more `\n` which is used in HTML to specify a new line. We can clean this up quickly with a little [character string manipulation](#string_manipulation).

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
library(magrittr)

scraping_wiki %>%
        html_nodes("#mw-content-text > div:nth-child(22)") %>% 
        html_text()

scraping_wiki %>%
        html_nodes("#mw-content-text > div:nth-child(22)") %>% 
        html_text() %>% 
        strsplit(split = "\n") %>%
        unlist() %>%
        .[. != ""]
```


Similarly, as we saw in our example above with scraping the main body content (`body_text`), there are extra characters (i.e. `\n`, `\`, `^`) in the text that we may not want.  Using a [little regex](#regex) we can clean this up so that our character string consists of only text that we see on the screen and no additional HTML code embedded throughout the text.

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
library(stringr)

# read the last 700 characters
substr(body_text, start = nchar(body_text)-700, stop = nchar(body_text))

# clean up text
body_text %>%
        str_replace_all(pattern = "\n", replacement = " ") %>%
        str_replace_all(pattern = "[\\^]", replacement = " ") %>%
        str_replace_all(pattern = "\"", replacement = " ") %>%
        str_replace_all(pattern = "\\s+", replacement = " ") %>%
        str_trim(side = "both") %>%
        substr(start = nchar(body_text)-700, stop = nchar(body_text))
```


So there we have it, text scraping in a nutshell.  Although not all encompassing, this section covered the basics of scraping text from HTML documents. Whether you want to scrape text from all common text-containing nodes such as `<div>`, `<p>`, `<ul>` and the like or you want to scrape from a specific node using the specific ID, this section provides you the basic fundamentals of using `rvest` to scrape the text you need. In the next section we move on to scraping data from HTML tables.



## Scraping HTML table data {#scraping_HTML_tables}
Another common structure of information storage on the Web is in the form of HTML tables. This section reiterates some of the information from the [previous section](#scraping_HTML_text); however, we focus solely on scraping data from HTML tables. The simplest approach to scraping HTML table data directly into R is by using either the [`rvest` package](#scraping_tables_rvest)  or the [`XML` package](#scraping_tables_xml).  To illustrate, I will focus on the [BLS employment statistics webpage](http://www.bls.gov/web/empsit/cesbmart.htm) which contains multiple HTML tables from which we can scrape data.

### Scraping HTML tables with rvest {#scraping_tables_rvest}
Recall that HTML elements are written with a start tag, an end tag, and with the content in between: `<tagname>content</tagname>`. HTML tables are contained within `<table>` tags; therefore, to extract the tables from the BLS employment statistics webpage we first use the `html_nodes()` function to select the `<table>` nodes.  In this case we are interested in all table nodes that exist on the webpage. In this example, `html_nodes` captures 15 HTML tables. This includes data from the 10 data tables seen on the webpage but also includes data from a few additional tables used to format parts of the page (i.e. table of contents, table of figures, advertisements).

```{r, collapse=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
library(rvest)

webpage <- read_html("http://www.bls.gov/web/empsit/cesbmart.htm")

tbls <- html_nodes(webpage, "table")

head(tbls)
```

Remember that `html_nodes()` does not parse the data; rather, it acts as a CSS selector. To parse the HTML table data we use `html_table()`, which would create a list containing 15 data frames.  However, rarely do we need to scrape *every* HTML table from a page, especially since some HTML tables don't catch any information we are likely interested in (i.e. table of contents, table of figures, footers). 

More often than not we want to parse specific tables. Lets assume we want to parse the second and third tables on the webpage:

1. Table 2. Nonfarm employment benchmarks by industry, March 2014 (in thousands) and
2. Table 3. Net birth/death estimates by industry supersector, April â€“ December 2014 (in thousands) 

This can be accomplished two ways. First, we can assess the previous `tbls` list and try to identify the table(s) of interest. In this example it appears that `tbls` list items 3 and 4 correspond with Table 2 and Table 3, respectively. We can then subset the list of table nodes prior to parsing the data with `html_table()`. This results in a list of two data frames containing the data of interest.

```{r, collapse=TRUE}
library(rvest)

webpage <- read_html("http://www.bls.gov/web/empsit/cesbmart.htm")

# subset list of table nodes for items 3 & 4
tbls_ls <- webpage %>%
        html_nodes("table") %>%
        .[3:4] %>%
        html_table(fill = TRUE)

str(tbls_ls)
```

An alternative approach, which is more explicit, is to use the [element selector process described in the previous section](#scraping_specific_nodes) to call the table ID name. 

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
# empty list to add table data to
tbls2_ls <- list()

# scrape Table 2. Nonfarm employment...
tbls2_ls$Table1 <- webpage %>%
        html_nodes("#Table2") %>% 
        html_table(fill = TRUE) %>%
        .[[1]]

# Table 3. Net birth/death...
tbls2_ls$Table2 <- webpage %>%
        html_nodes("#Table3") %>% 
        html_table() %>%
        .[[1]]

str(tbls2_ls)

```

One issue to note is when using `rvest`'s `html_table()` to read a table with split column headings as in *Table 2. Nonfarm employment...*.  `html_table` will cause split headings to be included and can cause the first row to include parts of the headings.  We can see this with Table 2.  This requires a little clean up.

```{r, collapse=TRUE, warning=FALSE, message=FALSE}

head(tbls2_ls[[1]], 4)

# remove row 1 that includes part of the headings
tbls2_ls[[1]] <- tbls2_ls[[1]][-1,]

# rename table headings
colnames(tbls2_ls[[1]]) <- c("CES_Code", "Ind_Title", "Benchmark",
                            "Estimate", "Amt_Diff", "Pct_Diff")

head(tbls2_ls[[1]], 4)

```


### Scraping HTML tables with XML {#scraping_tables_xml}
An alternative to `rvest` for table scraping is to use the [`XML`](https://cran.r-project.org/web/packages/XML/index.html) package. The XML package provides a convenient `readHTMLTable()` function to extract data from HTML tables in HTML documents.  By passing the URL to `readHTMLTable()`, the data in each table is read and stored as a data frame.  In a situation like our running example where multiple tables exists, the data frames will be stored in a list similar to `rvest`'s `html_table`.

```{r, collapse=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
library(XML)

url <- "http://www.bls.gov/web/empsit/cesbmart.htm"

# read in HTML data
tbls_xml <- readHTMLTable(url)

typeof(tbls_xml)

length(tbls_xml)
```

You can see that `tbls_xml` captures the same 15 `<table>` nodes that `html_nodes` captured. To capture the same tables of interest we previously discussed (*Table 2. Nonfarm employment...* and *Table 3. Net birth/death...*) we can use a couple approaches. First, we can assess `str(tbls_xml)` to identify the tables of interest and perform normal [list subsetting](#lists_subset). In our example list items 3 and 4 correspond with our tables of interest.

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
head(tbls_xml[[3]])

head(tbls_xml[[4]], 3)
```

Second, we can use the `which` argument in `readHTMLTable()` which restricts the data importing to only those tables specified numerically.

```{r, collapse=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
# only parse the 3rd and 4th tables
emp_ls <- readHTMLTable(url, which = c(3, 4))

str(emp_ls)
```

The third option involves explicitly naming the tables to parse.  This process uses the [element selector process described in the previous section](#scraping_specific_nodes) to call the table by name. We use `getNodeSet()` to select the specified tables of interest. However, a key difference here is rather than copying the table ID names you want to copy the XPath.  You can do this with the following: After you've highlighted the table element of interest with the element selector, right click the highlighted element in the developer tools window and select Copy XPath. From here we just use `readHTMLTable()` to convert to data frames and we have our desired tables.

```{r, collapse=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
library(RCurl)
library(XML)

# parse url
url_parsed <- htmlParse(getURL(url), asText = TRUE)

# select table nodes of interest
tableNodes <- getNodeSet(url_parsed, c('//*[@id="Table2"]', '//*[@id="Table3"]'))

# convert HTML tables to data frames
bls_table2 <- readHTMLTable(tableNodes[[1]])
bls_table3 <- readHTMLTable(tableNodes[[2]])

head(bls_table2)

head(bls_table3, 3)
```

A few benefits of `XML`'s `readHTMLTable` that are routinely handy include:

- We can specify names for the column headings
- We can specify the classes for each column
- We can specify rows to skip

For instance, if you look at `bls_table2` above notice that because of the split column headings on *Table 2. Nonfarm employment...* `readHTMLTable` stripped and replaced the headings with generic names because R does not know which variable names should align with each column. We can correct for this with the following:

```{r, collapse=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
library(XML) 

bls_table2 <- readHTMLTable(tableNodes[[1]], 
                            header = c("CES_Code", "Ind_Title", "Benchmark",
                            "Estimate", "Amt_Diff", "Pct_Diff"))

head(bls_table2)
```

Also, for `bls_table3` note that the net birth/death values parsed have been converted to factor levels.  We can use the `colClasses` argument to correct this.  

```{r, collapse=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
library(XML)

str(bls_table3)

bls_table3 <- readHTMLTable(tableNodes[[2]], 
                            colClasses = c("character","character", 
                                           rep("integer", 10)))

str(bls_table3)
```

Between `rvest` and `XML`, scraping HTML tables is relatively easy once you get fluent with the syntax and the available options.  This section covers just the basics of both these packages to get you moving forward with scraping tables. In the next section we move on to working with application program interfaces (APIs) to get data from the web.

[^selector]: You can learn more about selectors at [flukeout.github.io](http://flukeout.github.io/)

[^selector2]: You can simply assess the name of the ID in the highlighted element or you can  right click the highlighted element in the developer tools window and select *Copy selector*.  You can then paste directly into `html_nodes() as it will paste the exact ID name that you need for that element.



